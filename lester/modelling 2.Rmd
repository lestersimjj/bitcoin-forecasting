---
title: "Modelling with TidyModels"
author: "Lester Sim"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: no
editor_options:
  chunk_output_type: console
---

# 1. Load Packages
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#",
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  fig.align = "center",
  class.source = 'white'
)

# Set knit directory to project directory
```

```{r}
library(tidyverse)  # Data manipulation
library(tidymodels) # Building machine learning workflows and models
library(Quandl) # Download datasets
library(tidyquant)  # Functions for collecting and analyzing financial data
library(timetk)  # Functions to visualize, wrangle, and feature engineer time series data for forecasting and machine learning prediction
library(xgboost) # Machine learning algos
library(vip) # For constructing variable importance plots
library(caret) # Confusion matrix
library(stacks) # Ensemble
```

```{r}
# Set dates for this project. YMD Format.
sDate = "2018-01-01"
eDate = "2020-11-30"
```


# 2. Extracting Data/Adding features
## 2.1 Getting Bitcoin Prices
```{r}
# Get Bitcoin prices from Quandl. YMD.
# Quandl.api_key("5ydoG6gTCKjgzDpJp_1s") # 3GAtxPrAgoah7PyADPGy
# bitcoin_price <- Quandl("BCHARTS/BITSTAMPUSD", start_date=sDate, end_date=eDate) %>%
#   arrange(Date) %>%
#   as_tibble()
# colnames(bitcoin_price) <- c("date", "open", "high", "low", "close", "volume_btc", "volume_currency", "weighted_price")
# write_csv(bitcoin_price, 'data/bitcoin_price.csv')

# Read Data
bitcoin_price <- read_csv("data/bitcoin_price.csv")

# Cleaning
bitcoin_price[bitcoin_price == 0] <- NA
bitcoin_price <- bitcoin_price %>%
  map_df(na.locf)

# Defining Target
bitcoin_model <- bitcoin_price %>%
  select(date, close, volume_btc, volume_currency) %>% 
  tq_mutate(select = close,
            mutate_fun = periodReturn,
            period = 'daily',
            type = 'arithmetic',
            col_rename = 'future_return') %>%
  mutate(future_return_sign = as.factor(ifelse(future_return > 0, 1, 0))) %>% 
  # Shift up future_returns and sign by 1. Use today's data to predict tomorrow's returns.
  # future_returns indicates the returns if I buy at today's close and sell at tmrrw's close
  # each row represent today
  mutate_at(c("future_return", "future_return_sign"), lead)
bitcoin_model <- bitcoin_model[-nrow(bitcoin_model), ]

rmarkdown::paged_table(bitcoin_model %>% head())
```

## 2.2 Getting Bitcoin Features
```{r}
# Importing features from online sources (Marcus)
bitcoin_features <- read_csv("data/bitcoin_features.csv")
# Add sentiment features from reddit (Darren)
bitcoin_features <- bitcoin_features %>% 
  left_join(read_csv("data/bitcoin_reddit.csv"), by="date")
# Add sentiment features from news sites (QL)
bitcoin_features <- bitcoin_features %>% 
  left_join(read_csv("data/bitcoin_news.csv") %>% mutate(date = as.Date(date, format = "%d/%m/%y")), by="date")

# Remove lastest date to match bitcoin prices and future returns
bitcoin_features <- bitcoin_features[-nrow(bitcoin_features), ]

rmarkdown::paged_table(bitcoin_features %>% head())
```


## 2.3 Merge all data into 1 tibble
```{r}
# Combine price data with features
bitcoin_model <- bitcoin_model %>%
  left_join(bitcoin_features, by="date")

rmarkdown::paged_table(bitcoin_model %>% head())
```


# 3. Train/Test Data Split
![](lester/img/resampling.png)
```{r}
set.seed(123)
# YMD format
# No train/test split
# train <- bitcoin_model %>% filter(between(date, as.Date("2018-01-01"), as.Date("2020-06-30")))
# test <- bitcoin_model %>% filter(between(date, as.Date("2020-07-01"), as.Date("2020-11-29")))
```

# 4. Create Recipe
```{r}
# Recipe using the whole dataset
recipe_spec <- recipe(future_return_sign ~ ., data = bitcoin_model) %>%
  update_role(date, future_return, close, new_role = "ID") # Exclude these columns from the model. Only used for ID purpose

```

# 5. Create resamples from cross validation folds of 3 months
```{r}
# Cross Validation Folds for Tuning. Produces 9 slices
resamples_cv <- recipe_spec %>%
  prep() %>%
  juice() %>% 
  time_series_cv(
    date_var = date,
    # initial = '3 month',  # No. of data points from original data that are in analysis (training set)
    assess = '1 month',   # No. of data points from original data that are in assessment (testing set)
    skip = '1 month',     # Increment of resampling data set on assessment set only.
    slice_limit = 12,    # Ie. num of folds
    cumulative = TRUE    # TRUE, Analysis set will grow as resampling continues. Assessment set size remains static
  )

# Plot resampling timeline for each fold
resamples_cv %>% 
  plot_time_series_cv_plan(
        date, close, # date variable and value variable
        .facet_ncol = 2,
        .line_alpha = 0.5,
        .interactive = FALSE
    )
```

# 6. Define Model
```{r}
rf_model <- boost_tree(learn_rate = 0.01,
                        tree_depth = tune(),
                        min_n = 1,
                        mtry = 500,
                        trees = tune(),
                        stop_iter = 50) %>%
  set_engine('xgboost') %>%
  set_mode('classification')

rf_model
```

# 7. Create Workflow
```{r}
rf_wflw <- workflow() %>%
  add_recipe(recipe_spec) %>%
  add_model(rf_model)

rf_wflw
```

# 8. Tuning
## 8.1 Create grid for tuning.
```{r}
# Create grid. Create 5^2 = 25 models.
rf_grid <- grid_regular(tree_depth(),
                         trees(),
                         levels = 5)
rf_grid
```

## 8.2 Model Tuning with a Grid
```{r}
# Train the model using cross-validation
rf_model_trained <- rf_wflw %>% tune_grid(
  grid = rf_grid,
  metrics = metric_set(accuracy, roc_auc),
  resamples = resamples_cv,
  control = control_resamples(verbose = FALSE,
                              save_pred = TRUE,
                              allow_par = TRUE))

rf_model_results <- rf_model_trained %>% 
  collect_metrics()

# Show the top 5 candidate models
rf_model_trained %>% 
  show_best("accuracy")

# Save the best model parameters based on mn_log_loss metric
best_params <- rf_model_trained %>%
  select_best('accuracy')
```

# 9. Finalize workflow with the best model parameters
```{r}
rf_wflw_best <- rf_wflw %>% 
  finalize_workflow(best_params)

rf_wflw_best
```

# 10. Fit Model with Resampling (on Entire Set)
```{r}
# Used as an alternative to measuring the model's performance
# 9 Folds from previously defined resamples
rf_resampling <- rf_wflw_best %>%
  fit_resamples(
    resamples = resamples_cv,
    # Save predictions and model from each split
    control   = control_resamples(save_pred = TRUE, 
                                  extract = function(x) extract_model(x)))

rf_resampling %>% 
  collect_metrics()

```

# 11. Extracting and Visualizing variable importance
See changes in variable importance overtime.
```{r}


# Plot variable importance of each split
library(gridExtra)
rf_resampling_vip_plot <- lapply(12:1, function(.x) 
  rf_resampling$.extracts[[.x]]$.extracts[[1]] %>% vip(num_features = 5) + ylab(rf_resampling$id[[.x]]))
grid.arrange(grobs = rf_resampling_vip_plot, ncol = 6, nrow = 2)
```

# 14. Evaluating Models
## 14.1 Computing Returns from the Model.  
Strategy 1: Buy and Hold  
Strategy 2: Using time series model - Prophet
Strategy 3: Using Random Forest, XGBoost

```{r}
# Function to compute returns. Returns a 1 column vector
# predicted should be a 2 column df with columns date, pred_signal
# Assume 0.3% as trading cost
computeReturns <- function(base, predicted){
  overall <- base %>% 
    left_join(predicted, by="date") %>% 
    mutate(trading_cost = abs(pred_signal - lag(pred_signal, n = 1, default = 0)) * 0.003,
           return_model = cumprod(1 + future_return * pred_signal - trading_cost))
  return(overall$return_model)
}

# Base Model
# Combines all the assessment sets from the 12 splits from cross-validation resamples
base_model <- resamples_cv %>% 
  mutate(.testing = map(splits, testing)) %>% 
  select(id, .testing) %>% 
  unnest(.testing) %>% 
  arrange(date) %>% 
  select(date, close, future_return_sign, future_return) %>% 
  mutate(return_buyhold = cumprod(1 + future_return),
         signal_buyhold = 1)

# RF model
# Extract and merge predictions of individual fold test data
rf_predict <- rf_resampling %>%
  mutate(.assessment = map(splits, assessment)) %>%
  select(id, .predictions, .assessment) %>% 
  unnest(c(.predictions, .assessment), names_repair = "universal") %>% 
  rename(pred_signal = .pred_class) %>% 
  arrange(date) %>% 
  select(date, pred_signal) %>% 
  mutate_at("pred_signal", ~as.numeric(as.character(.)))  # Convert factor into num

# Time Series
# prophet_predict <- read_csv("data/prophet_signals.csv") %>% 
#   filter(date < "2020-11-30") %>%
#   rename("pred_signal" = "prophet_pred_signal") %>% 
#   select(-date)

all_models <- base_model %>% 
  # mutate(signal_prophet = prophet_predict$pred_signal,
  #        return_prophet = computeReturns(base_model, prophet_predict),
  #        signal_rf = rf_predict$pred_signal,
  #        return_rf = computeReturns(base_model, rf_predict))
  mutate(signal_rf = rf_predict$pred_signal,
         return_rf = computeReturns(base_model, rf_predict))

rmarkdown::paged_table(all_models %>% head())
```

## 14.2 Visualisations
```{r}
# Plot predicted buy/sell signal on price data
# See differences between models
prophet_plot <- all_models %>% ggplot(aes(x = date, y = close, color = signal_prophet)) +
    geom_line() +
    theme_light()
rf_plot <- all_models %>% ggplot(aes(x = date, y = close, color = signal_rf)) +
    geom_line() +
    theme_light()
grid.arrange(prophet_plot, rf_plot, nrow = 1)
```

```{r}
# Comparing returns across strategies
all_models %>% 
  select(date, return_buyhold, return_rf) %>% 
  # select(date, return_buyhold, return_prophet, return_rf) %>% 
  gather(key = "strategy", value = "returns", -date) %>% 
  ggplot(aes(x=date, y = returns)) +
    geom_line(aes(color = strategy)) +
    theme_light()
```


## 14.3 Descriptive Statistics
### 14.3.1 Confusion Matrix
```{r}
# Confusion Matrix for Prohphet
print(confusionMatrix(factor(all_models$signal_prophet),
                factor(all_models$future_return_sign)))

# Confusion Matrix for Random Forest
print(confusionMatrix(factor(all_models$signal_rf),
                factor(all_models$future_return_sign)))
```

### 14.3.2 Overall Returns
```{r}
all_returns <- all_models %>% 
  select(date, return_buyhold, return_rf) %>% 
  # select(date, return_buyhold, return_prophet, return_rf) %>% 
  gather(key = "strategy", value = "returns", -date) %>% 
  group_by(strategy) %>% 
  summarise_all(last)

rmarkdown::paged_table(all_returns)
```

### 14.3.3 Mean Daily Returns and SD
```{r}
# Daily Returns
all_models_performance <- all_models %>% 
  select(date, future_return, signal_buyhold, signal_prophet, signal_rf) %>% 
  gather(key = "strategy", value = "daily_returns", -c(date, future_return)) %>% 
  mutate_at("daily_returns", ~.*future_return) %>% 
  group_by(strategy) %>% 
  summarise(d_ret = mean(daily_returns), d_sd = sd(daily_returns))

rmarkdown::paged_table(all_models_performance)
```

### 14.3.4 Annualised Returns and SD
```{r}
all_models_performance <- all_models_performance %>% 
  mutate(ann_ret = (1+d_ret)^365 - 1,
         ann_sd = d_sd * 365 ^ 0.5)
rmarkdown::paged_table(all_models_performance)
```

### 14.3.4 Sharpe Ratio
```{r}
all_models_performance <- all_models_performance %>% 
  # Minus risk-free rate. T-bills
  mutate(ann_sharpe = ann_ret-0.0011/ann_sd)

rmarkdown::paged_table(all_models_performance)
```

