---
title: "Modelling with TidyModels"
author: "Lester Sim"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: no
editor_options:
  chunk_output_type: console
---

# 1. Load Packages
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#",
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  fig.align = "center",
  class.source = 'white'
)

# Set knit directory to project directory
```

```{r}
library(tidyverse)  # Data manipulation
library(tidymodels) # Building machine learning workflows and models
library(Quandl) # Download datasets
library(tidyquant)  # Functions for collecting and analyzing financial data
library(timetk)  # Functions to visualize, wrangle, and feature engineer time series data for forecasting and machine learning prediction
library(xgboost) # Machine learning algos
library(vip) # For constructing variable importance plots
library(caret) # Confusion matrix
```

```{r}
# Set dates for this project. YMD Format.
sDate = "2018-01-01"
eDate = "2020-11-30"
```


# 2. Extracting Data/Adding features
## 2.1 Getting Bitcoin Prices
```{r}
# Get Bitcoin prices from Quandl. YMD.
# Quandl.api_key("5ydoG6gTCKjgzDpJp_1s") # 3GAtxPrAgoah7PyADPGy
# bitcoin_price <- Quandl("BCHARTS/BITSTAMPUSD", start_date=sDate, end_date=eDate) %>%
#   arrange(Date) %>%
#   as_tibble()
# colnames(bitcoin_price) <- c("date", "open", "high", "low", "close", "volume_btc", "volume_currency", "weighted_price")
# write_csv(bitcoin_price, 'data/bitcoin_price.csv')

# Read Data
bitcoin_price <- read_csv("data/bitcoin_price.csv")

# Cleaning
bitcoin_price[bitcoin_price == 0] <- NA
bitcoin_price <- bitcoin_price %>%
  map_df(na.locf)

# Defining Target
bitcoin_model <- bitcoin_price %>%
  select(date, close, volume_btc, volume_currency) %>% 
  tq_mutate(select = close,
            mutate_fun = periodReturn,
            period = 'daily',
            type = 'arithmetic',
            col_rename = 'future_return') %>%
  mutate(future_return_sign = as.factor(ifelse(future_return > 0, 1, 0))) %>% 
  # Shift up future_returns and sign by 1. Use today's data to predict tomorrow's returns.
  # future_returns indicates the returns if I buy at today's close and sell at tmrrw's close
  # each row represent today
  mutate_at(c("future_return", "future_return_sign"), lead)
bitcoin_model <- bitcoin_model[-nrow(bitcoin_model), ]

rmarkdown::paged_table(bitcoin_model %>% head())
```

## 2.2 Getting Bitcoin Features
```{r}
# Importing features from online sources (Marcus)
bitcoin_features <- read_csv("data/bitcoin_features.csv")
# Add sentiment features from reddit (Darren)
bitcoin_features <- bitcoin_features %>% 
  left_join(read_csv("data/bitcoin_reddit.csv"), by="date")

# Remove lastest date to match bitcoin prices and future returns
bitcoin_features <- bitcoin_features[-nrow(bitcoin_features), ]

rmarkdown::paged_table(bitcoin_features %>% head())
```


## 2.3 Merge all data into 1 tibble
```{r}
# Combine price data with features
bitcoin_model <- bitcoin_model %>%
  left_join(bitcoin_features, by="date")

rmarkdown::paged_table(bitcoin_model %>% head())
```


# 3. Train/Test Data Split
```{r}
set.seed(123)
# YMD format
train <- bitcoin_model %>% filter(between(date, as.Date("2018-01-01"), as.Date("2020-06-30")))
test <- bitcoin_model %>% filter(between(date, as.Date("2020-07-01"), as.Date("2020-11-29")))
```

# 4. Create Recipe
```{r}
recipe_spec <- recipe(future_return_sign ~ ., data = train) %>%
  update_role(date, future_return, close, new_role = "ID") # Exclude these columns from the model. Only used for ID purpose

```

# 5. Create resamples from cross validation folds of 3 months
```{r}
# Cross Validation Folds for Tuning. Produces 7 folds.
resamples_cv <- recipe_spec %>%
  prep() %>%
  juice() %>% 
  time_series_cv(
    date_var = date,
    initial = '3 month',  # No. of data points from original data that are in analysis (training set)
    assess = '3 month',   # No. of data points from original data that are in assessment (testing set)
    skip = '3 month',     # Increment of resampling data set on assessment set only.
    cumulative = TRUE    # TRUE, Analysis set will grow as resampling continues. Assessment set size remains static
  )

# Plot resampling timeline for each fold
resamples_cv %>% 
  plot_time_series_cv_plan(
        date, close, # date variable and value variable
        .facet_ncol = 2,
        .line_alpha = 0.5,
        .interactive = FALSE
    )
```

# 6. Define Model
```{r}
xgb_model <- boost_tree(learn_rate = 0.01,
                        tree_depth = 1,
                        min_n = 1,
                        mtry = 500,
                        trees = tune(),
                        stop_iter = 50) %>%
  set_engine('xgboost') %>%
  set_mode('classification')

xgb_model
```

# 7. Create Workflow
```{r}
xgb_wflw <- workflow() %>%
  add_recipe(recipe_spec) %>%
  add_model(xgb_model)

xgb_wflw
```

# 8. Tuning
## 8.1 Create grid for tuning.
```{r}
# Create grid. Create 10 models.
xgb_params <- grid_max_entropy(trees(), size = 10)
xgb_params
```

## 8.2 Model Tuning with a Grid
```{r}
# For each fold, pass through all possible models generated from grid size
# Each row represents one fold
# In each fold, no. of models x no. of metrics
# In each fold, for .predictions, no. of models x length of test data
xgb_model_trained <- xgb_wflw %>% tune_grid(
  grid = xgb_params,
  metrics = metric_set(accuracy, roc_auc, mn_log_loss),  # Mean log loss
  resamples = resamples_cv,
  control = control_resamples(verbose = FALSE,
                              save_pred = TRUE,
                              allow_par = TRUE))

# Display results in a tidy tibble
# Each model would be ran using the cross-fold validation. Average performance statstics reported back.
# Note that the cross-fold validation models are not kept. Only using cross-fold validation to calculate performance statistics for us to choose which model parameters to use.
# no. of models x no. of metrics
xgb_model_results <- xgb_model_trained %>% 
  collect_metrics()

# Show the top 5 candidate models
xgb_model_trained %>% 
  show_best("mn_log_loss")

# Save the best model parameters based on mn_log_loss metric
best_params <- xgb_model_trained %>%
  select_best('mn_log_loss', maximise = FALSE)
```

# 9. Finalize workflow with the best model parameters
```{r}
xgb_wflw_best <- xgb_wflw %>% 
  finalize_workflow(best_params)

xgb_wflw_best
```

# 10. Fitting final model to training data
```{r}
xgb_final <- 
  xgb_wflw_best %>%
  fit(data = train)

print(xgb_final)

# Training set predictions
xgb_final_training_pred <- 
  predict(xgb_final, train) %>% 
  # Add in predicted class probabilities
  bind_cols(predict(xgb_final, train, type = "prob")) %>% 
  # Add the true outcome data back in
  bind_cols(train %>% select(future_return_sign))

# Get accuracy from predicting using training set
xgb_final_training_pred %>%
  accuracy(truth = future_return_sign, .pred_class)

```

# 11. Extracting and Visualizing variable importance
```{r}
# Extract model object from final workflow. 
# Visualise variable importance using vip()
xgb_final %>% 
  pull_workflow_fit() %>% 
  vip()
```

# 12. Fit Model with Resampling instead of training on the whole train set.
```{r}
# Used as an alternative to measuring the model's performance
# 7 Folds from previously defined resamples
xgb_resampling <- xgb_wflw_best %>%
  fit_resamples(
    resamples = resamples_cv,
    control   = control_resamples(
      verbose   = FALSE,  # Logging results as they are generated
      save_pred = TRUE,   # Save the out-of-samples predictions for each model evaluated
      allow_par = TRUE))  # Allow parallel processing

# Return the average performance of the model based on training and testing different folds
xgb_resampling %>% collect_metrics()

```

# 13. Fitting final model to testing data (Last Step)
```{r}
# Testing set predictions
# Using the model that was trained with all the training data and fitting onto the never seen before test data
xgb_final_testing_pred <- 
  predict(xgb_final, test) %>% 
  # Add in predicted class probabilities
  bind_cols(predict(xgb_final, test, type = "prob")) %>% 
  # Add the true outcome data back in
  bind_cols(test %>% select(future_return_sign))

# Get accuracy from predicting using testing set
xgb_final_testing_pred %>%
  accuracy(truth = future_return_sign, .pred_class)
```


# 14. Evaluating Model
## 14.1 Computing Returns from the Model.  
Strategy 1: Buy and Hold  
Strategy 2: Using the model's predicted signal to buy/sell at the closing price  
```{r}
# Train and Test set predictions
# Using the model that was trained with all the training data and fitting onto the entire dataset
xgb_final_full_pred <- bitcoin_model %>% 
  select(date, close, future_return_sign, future_return) %>% 
  # Add in predicted
  bind_cols(predict(xgb_final, bitcoin_model)) %>% 
  rename(pred_signal = .pred_class) %>% 
  mutate_at("pred_signal", ~as.numeric(as.character(.))) %>%  # Convert factor to binary integer
  mutate(trading_cost = abs(pred_signal - lag(pred_signal, n = 1, default = 0)) * 0.003,
         return_buyhold = cumprod(1 + future_return),
         return_model = cumprod(1 + future_return * pred_signal - trading_cost))
```

## 14.2 Visualisations
```{r}
# Plot predicted buy/sell signal on price data
xgb_final_full_pred %>% 
  ggplot(aes(x = date, y = close, color = pred_signal)) + 
    geom_line() +
    theme_light()

# Comparing buyhold v model returns
xgb_final_full_pred %>% 
  select(date, return_buyhold, return_model) %>% 
  gather(key = "strategy", value = "returns", -date) %>% 
  ggplot(aes(x=date, y = returns)) +
    geom_line(aes(color = strategy)) +
    theme_light()
```

## 14.3 Descriptive Statistics
```{r}
# Confusion Matrix
confusionMatrix(factor(xgb_final_full_pred$pred_signal),
                factor(xgb_final_full_pred$future_return_sign))
```

