---
title: "Modelling with TidyModels"
author: "Lester Sim"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: no
editor_options:
  chunk_output_type: console
---

# 1. Load Packages
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#",
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  fig.align = "center",
  class.source = 'white'
)

# Set knit directory to project directory
```

```{r}
library(tidyverse)  # Data manipulation
library(tidymodels) # Building machine learning workflows and models
library(Quandl) # Download datasets
library(tidyquant)  # Functions for collecting and analyzing financial data
library(timetk)  # Functions to visualize, wrangle, and feature engineer time series data for forecasting and machine learning prediction
library(xgboost) # Machine learning algos
library(vip) # For constructing variable importance plots
library(caret) # Confusion matrix
library(stacks) # Ensemble
```

```{r}
# Set dates for this project. YMD Format.
sDate = "2018-01-01"
eDate = "2020-11-30"
```


# 2. Extracting Data/Adding features
## 2.1 Getting Bitcoin Prices
```{r}
# Read Data
bitcoin_price <- read_csv("data/bitcoin_price.csv")

# Cleaning
bitcoin_price[bitcoin_price == 0] <- NA
bitcoin_price <- bitcoin_price %>%
  map_df(na.locf)

# Defining Target
bitcoin_model <- bitcoin_price %>%
  select(date, close, volume_btc, volume_currency) %>% 
  tq_mutate(select = close,
            mutate_fun = periodReturn,
            period = 'daily',
            type = 'arithmetic',
            col_rename = 'future_return') %>%
  mutate(future_return_sign = as.factor(ifelse(future_return > 0, 1, 0))) %>% 
  mutate_at(c("future_return", "future_return_sign"), lead)
bitcoin_model <- bitcoin_model[-nrow(bitcoin_model), ]

rmarkdown::paged_table(bitcoin_model %>% head())
```

## 2.2 Getting Bitcoin Features
```{r}
# Importing features from online sources (Marcus)
bitcoin_features <- read_csv("data/bitcoin_features.csv")
# Add sentiment features from reddit (Darren)
bitcoin_features <- bitcoin_features %>% 
  left_join(read_csv("data/bitcoin_reddit.csv"), by="date")
# Add sentiment features from news sites (QL)
bitcoin_features <- bitcoin_features %>% 
  left_join(read_csv("data/bitcoin_news.csv") %>% mutate(date = as.Date(date, format = "%d/%m/%y")), by="date")

# Remove lastest date to match bitcoin prices and future returns
bitcoin_features <- bitcoin_features[-nrow(bitcoin_features), ]


rmarkdown::paged_table(bitcoin_features %>% head())
```


## 2.3 Merge all data into 1 tibble
```{r}
# Combine price data with features
bitcoin_model <- bitcoin_model %>%
  left_join(bitcoin_features, by="date")

rmarkdown::paged_table(bitcoin_model %>% head())
```


# 3. Train/Test Data Split
```{r}
set.seed(123)
# YMD format
train <- bitcoin_model %>% filter(between(date, as.Date("2018-01-01"), as.Date("2020-06-30")))
test <- bitcoin_model %>% filter(between(date, as.Date("2020-07-01"), as.Date("2020-11-29")))
```

# 4. Create Recipe
```{r}
recipe_spec <- recipe(future_return_sign ~ ., data = train) %>%
  update_role(date, future_return, close, new_role = "ID")

```

# 5. Create resamples from cross validation folds of 3 months. Use this resample for all models.
```{r}
# Cross Validation Folds for Tuning. Produces 9 folds.
resamples_cv <- recipe_spec %>%
  prep() %>%
  juice() %>% 
  time_series_cv(
    date_var = date,
    initial = '3 month',  # No. of data points from original data that are in analysis (training set)
    assess = '3 month',   # No. of data points from original data that are in assessment (testing set)
    skip = '3 month',     # Increment of resampling data set on assessment set only.
    cumulative = TRUE    # TRUE, Analysis set will grow as resampling continues. Assessment set size remains static
  )

# Plot resampling timeline for each fold
resamples_cv %>% 
  plot_time_series_cv_plan(
        date, close, # date variable and value variable
        .facet_ncol = 2,
        .line_alpha = 0.5,
        .interactive = FALSE
    )
```

# 6. Convenience Functions
```{r}
# Convenience functions. Set save_pred = TRUE, save_workflow = TRUE
# To be used in tune_grid(control = ) later for each model
ctrl_grid <- control_stack_grid()
ctrl_res <- control_stack_resamples()
```

# 6. XGBoost Model
```{r}
# Define Model
xgb_model <- boost_tree(learn_rate = 0.01,
                        tree_depth = tune(),
                        min_n = 1,
                        mtry = 500,
                        trees = tune(),
                        stop_iter = 50) %>%
  set_engine('xgboost') %>%
  set_mode('classification')
xgb_model

# Define Grid
xgb_grid <- grid_regular(tree_depth(),
                         trees(),
                         levels = 5)

# Define Workflow
xgb_wflw <- workflow() %>%
  add_recipe(recipe_spec) %>%
  add_model(xgb_model)
xgb_wflw

# Perform tuning and cross-validation on training set
xgb_model_trained <- xgb_wflw %>% tune_grid(
  grid = xgb_grid,
  metrics = metric_set(accuracy, roc_auc, mn_log_loss),  # Mean log loss
  resamples = resamples_cv,
  control = ctrl_grid)
xgb_model_trained

xgb_model_metrics <- xgb_model_trained$.metrics[[1]] %>% 
  select(trees, tree_depth)

```

# 9. Neural Network Model
```{r}
# Define Model
nnet_spec <-
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
  set_mode("classification") %>%
  set_engine("nnet")

# Define Workflow
nnet_wflow <- workflow() %>%
  add_recipe(recipe_spec) %>% 
  add_model(nnet_spec)

# Perform tuning and cross-validation on training set
nnet_res <-
  tune_grid(
    object = nnet_wflow, 
    resamples = resamples_cv, 
    grid = 10,
    metrics = metric_set(accuracy, roc_auc, mn_log_loss),
    control = ctrl_grid
  )

```

# 10. Ensemble
```{r}
# Putting together a stack
# Models that are perfectly collinear to other candidate models will be removed
ensemble_st <- stacks() %>% 
  add_candidates(xgb_model_trained) %>% 
  add_candidates(nnet_res)

ensemble_st_tibble <- as_tibble(ensemble_st)
# Some rows from neural network are predicting NA. Not sure why.
# empty <- ensemble_st_tibble %>% filter_all(any_vars(is.na(.)))

# Combining outputs from stack members and give weightage (coefficients) to each stack member
# Test out a range of penalties and see which produces the best metric (accuracy)
# Higher penalty, lower number of models selected
ensemble_weight <- ensemble_st %>% 
  blend_predictions(penalty = seq(0, 1, 0.1))
ensemble_weight

# Plot the chart of penalty against average accuracy of ensemble model
autoplot(ensemble_weight)

# Get stacking coefficients
# Don't know why coef is NA here
collect_parameters(ensemble_weight, "nnet_res")

# Fit shortlisted models onto full training set
ensemble_model <- ensemble_weight %>% 
  fit_members()

# Make predictions with ensemble model on train data
ensemble_model_train <- train %>% 
  select(date, future_return, future_return_sign) %>% 
  bind_cols(predict(ensemble_model, train, type = "class"))

# Confusion Matrix
confusionMatrix(factor(ensemble_model_train$.pred_class),
                factor(ensemble_model_train$future_return_sign))

# Get Member's Predictions
train %>% 
  select(date, future_return, future_return_sign) %>% 
  bind_cols(predict(ensemble_model, train, type = "class", members = TRUE))
```

