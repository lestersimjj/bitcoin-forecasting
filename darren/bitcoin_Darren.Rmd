---
title: "Bitcoin Momentum Feature Based Forecasting"
author: "Nico"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: no
editor_options:
  chunk_output_type: console
---
# 1. Load Packages

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#",
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  fig.align = "center",
  class.source = 'white'
)
```

```{r}
library(tidyverse)
library(stringr)
library(gtrendsR)
library(Quandl)
library(quantmod)
library(RcppRoll)
library(lubridate)
library(tidyquant)
library(tidymodels)
library(tsfeatures)
library(slider)
library(timetk)
library(data.table)
library(caret)
library(mlbench)
```

#2.5 NLP for Reddit Threads & Data Manipulation

```{r, echo = FALSE}
#code for scraping reddit threads for bitcoin-related data, for sentiment analysis
library(RedditExtractoR)
bitcoin_urls = reddit_urls(search_terms="bitcoin")

bitcoin_urls
```

```{r, echo = FALSE}
#obtain stock price for Grayscale Bitcoin Trust (BTC), the only fund of its kind for Bitcoin especially
#dates can be changed (, from = '2017-01-01', to = "2018-03-01")
library(tidyquant)

options("getSymbols.warning4.0"=FALSE)
options("getSymbols.yahoo.warning"=FALSE)

getSymbols("GBTC",warnings = FALSE,
           auto.assign = TRUE)

head(GBTC)
chart_Series(GBTC)
```

```{r, echo = FALSE}

library(RedditExtractoR)

bitcoin_nlp = get_reddit(search_terms = 'bitcoin', regex_filter = "", subreddit = NA,
  cn_threshold = 1, page_threshold = 10, sort_by = "comments",
  wait_time = 2)



```

```{r, echo = FALSE}
dfnlp = subset(bitcoin_nlp, select = c(comm_date, comment) )

#write.csv(dfnlp,"C:\\Users\\Darren\\Desktop\\bitcointext.csv", row.names = FALSE)

head(dfnlp)
```

```{r, echo = FALSE}
library(lubridate)

date <- dmy(dfnlp$comm_date)

class(date)

dfnlp2 <- cbind(dfnlp, date) 

dfnlp3 <- subset(dfnlp2, select = c(date, comment) )

head(dfnlp3)

```

```{r, echo = FALSE}

dfnlp4 <- subset(dfnlp3, date > "2018-01-01") 

dfnlp5 <- dfnlp4[order(as.Date(dfnlp4$date, format="%d/%m/%Y")),]

head(dfnlp5)

```

```{r}
library(tidytext)
library(dplyr)

# count words, tokenize

reddit_token <- dfnlp5 %>%
  select(comment) %>%
  unnest_tokens(word, comment) %>%
  group_by(word) %>%
  summarize(n_tokens = n()) %>%
  arrange(desc(n_tokens))

reddit_token
```

```{r}
#unnnest comment column but keep date
reddit_token <- dfnlp5 %>%
  unnest_tokens(word, comment)

reddit_token
```

```{r}
# Process Stopwords
stopwords_sw_iso = stopwords::stopwords(language = 'en',source='stopwords-iso')
# Exlude Some Stopwords
excludefromstopwords <- c("high", "new", "up", "above", "back", "below", "big", "higher", "world", "down", "index", "interest", "billion", "early", "under", "changes", "highest", "lower", "lowest", "million", "states", "value", "microsoft", "website", "bottom", "best")
stopwords_sw_iso <- stopwords_sw_iso[!stopwords_sw_iso %in% excludefromstopwords]
# Include Some Stopwords
extra_stop_words <- c("day", "investing.com", "2.0", "2020")
# Include Some Stopwords
stop_words = data.frame(word=unique(c(stopwords_sw_iso,extra_stop_words)),stringsAsFactors = F)
stop_words = stop_words %>% mutate(stopword=1)
# Process Stopwords
reddit_token = reddit_token %>% 
  left_join(y=stop_words, by= "word", match = "all") %>%
  filter(is.na(stopword))
reddit_token

```


```{r}
#sentiment score - bing lexicon

sentiment_score <-
reddit_token %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(date, sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  mutate(sentiment=positive-negative)

sentiment_score

```

```{r}
#viewing of score in viewer
sentiment_score %>%
DT::datatable()
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

