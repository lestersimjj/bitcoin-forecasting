---
title: 'Bitcoin Feature Engineering - NLP of Reddit Comments'
author: "Darren Tan Jing Rong"
date: "`r Sys.Date()`"
output:
  html_document:
    #code_folding: show
    #by default want to show the code without option to hide
    number_sections: no #change from yes to no
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: no
#editor_options:
  #chunk_output_type: console
---

```{r setup, include=FALSE}
#
rm(list = ls())

# knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(
  collapse=TRUE,
  comment="#",
  message=FALSE,
  warning=FALSE,
  cache=FALSE,
  fig.align = "center",
  results="hold"
)

path_root="."
```

```{r, include=FALSE}
# Function for colouring our text in Rmarkdown
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}
```

```{r}
library(tidyverse)
library(stringr)
library(gtrendsR)
library(Quandl)
library(quantmod)
library(RcppRoll)
library(lubridate)
library(tidyquant)
library(tidymodels)
library(tsfeatures)
library(slider)
library(timetk)
library(data.table)
library(caret)
library(mlbench)
```

`r colorize("One of our feature engineering additions is to webscrape Reddit data for chatter about bitcoin.", "blue")`
`r colorize("This complements our scraping of technical data and news as reddit chatter measures sentiment and we can see the level of interest through performing sentiment analysis on the comments", "blue")`

`r colorize("The first step is to use the RedditExtractoR library's get_reddit command to scrape for all bitcoin related threads and comments", "blue")`
`r colorize("The function get_reddit combines both reddit_urls, used to extract URLs of Reddit threads, and reddit_content, used to extract comments from the threads identified using reddit_urls", "blue")`
`r colorize("Parameters have been slightly adjusted. The main search term is bitcoin", "blue")`


```{r}

library(RedditExtractoR)

bitcoin_nlp = get_reddit(search_terms = 'bitcoin', regex_filter = "", subreddit = NA,
  cn_threshold = 1, page_threshold = 10, sort_by = "comments",
  wait_time = 2)

```

`r colorize("The function outputs a dataframe, but has unecessary columns like the URL and the date of the post. We only require the date of the comment and the comment itself, so we use subset to extract these two columns", "blue")`

```{r}

colnames(bitcoin_nlp)
head(bitcoin_nlp)

```

```{r}
dfnlp = subset(bitcoin_nlp, select = c(comm_date, comment) )

#write.csv(dfnlp,"C:\\Users\\Darren\\Desktop\\bitcointext.csv", row.names = FALSE)

head(dfnlp)
```

`r colorize("The date column was not in datetime format, so we use the lubridate library to change it to datetime format", "blue")`

```{r}
library(lubridate)

date <- dmy(dfnlp$comm_date)

class(date)

dfnlp2 <- cbind(dfnlp, date) 

dfnlp3 <- subset(dfnlp2, select = c(date, comment) )

head(dfnlp3)

```

`r colorize("The dataset goes all the way back to 2013. We only require 2018 onwards - this is standardised across our features for our model so we just subset the portion we need", "blue")`

```{r}

dfnlp4 <- subset(dfnlp3, date > "2018-01-01") 

dfnlp5 <- dfnlp4[order(as.Date(dfnlp4$date, format="%d/%m/%Y")),]

head(dfnlp5)

```

`r colorize("Here we are starting to do NLP. The first step is to get an overview of the tokens", "blue")`

```{r}
library(tidytext)
library(dplyr)

# count words, tokenize

reddit_token <- dfnlp5 %>%
  select(comment) %>%
  unnest_tokens(word, comment) %>%
  group_by(word) %>%
  summarize(n_tokens = n()) %>%
  arrange(desc(n_tokens))

head(reddit_token)
```

`r colorize("Here we just unnest the comments column and obtain individual words", "blue")`

```{r}
#unnnest comment column but keep date
reddit_token <- dfnlp5 %>%
  unnest_tokens(word, comment)

head(reddit_token)
```

`r colorize("The next step is to remove stopwords from the tokenized dataset. We use stopwords-iso but can include and exclude stopwords as we require. Using left_join we then obtain a dataset with stopwords removed", "blue")`

```{r}
# Process Stopwords
stopwords_sw_iso = stopwords::stopwords(language = 'en',source='stopwords-iso')
# Exlude Some Stopwords
excludefromstopwords <- c("high", "new", "up", "above", "back", "below", "big", "higher", "world", "down", "index", "interest", "billion", "early", "under", "changes", "highest", "lower", "lowest", "million", "states", "value", "microsoft", "website", "bottom", "best")
stopwords_sw_iso <- stopwords_sw_iso[!stopwords_sw_iso %in% excludefromstopwords]
# Include Some Stopwords
extra_stop_words <- c("day", "investing.com", "2.0", "2020")
# Include Some Stopwords
stop_words = data.frame(word=unique(c(stopwords_sw_iso,extra_stop_words)),stringsAsFactors = F)
stop_words = stop_words %>% mutate(stopword=1)
# Process Stopwords
reddit_token = reddit_token %>% 
  left_join(y=stop_words, by= "word", match = "all") %>%
  filter(is.na(stopword))
head(reddit_token)

```

`r colorize("Here we obtain a sentiment score using the bing lexicon, which gives us a sentiment by counting positive and negative words for each day and taking the difference to give a sentiment score", "blue")`

`r colorize("However, the problem is that we still have days with no reddit chatter, resulting in gaps between the dates", "blue")`

```{r}
#sentiment score - bing lexicon

sentiment_score <-
reddit_token %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(date, sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  mutate(sentiment=positive-negative)

head(sentiment_score)

```

`r colorize("We rectify the problem by using the complete function to fill in empty dates and then replacing all the NA values with 0", "blue")`
`r colorize("This is because we want days with no bitcoin chatter to have 0 sentiment", "blue")`
`r colorize("Now, the dataset is in a clean format for our model. All 3 columns can be used as features - negative, positive and overall sentiment", "blue")`


```{r}
sentiment_score <-
sentiment_score %>%
  complete(date = seq.Date(as.Date("2018/1/1"), as.Date("2020/11/30"), by="day")) 

sentiment_score[is.na(sentiment_score)] <- 0

head(sentiment_score)

```

`r colorize("Renamed the columns so it does not conflict with the news NLP section", "blue")`

```{r}
colnames(sentiment_score)
sentiment_score <- sentiment_score %>% rename(reddit_overall_sentiment = sentiment) 
sentiment_score <- sentiment_score %>% rename(reddit_negative_sentiment = negative) 
sentiment_score <- sentiment_score %>% rename(reddit_positive_sentiment = positive) 

head(sentiment_score)
```

```{r}
#just for viewing in data table
sentiment_score %>%
DT::datatable()
```

```{r}
#export results as csv
write.csv(sentiment_score,"C:\\Users\\Darren\\Desktop\\bitcoin_reddit.csv", row.names = FALSE)

```

```{r}

```

```{r}

```

